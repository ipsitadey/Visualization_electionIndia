{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing python libraries\n",
    "1. To clean up the data. ex. removing stopwords, lemmentizing, tokenizing of tweet etc.\n",
    "2. Geocoder API to extract the formatted address and (longitude, latitude) for the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hadoop/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "import csv\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from autocorrect import spell\n",
    "import itertools\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to preprocess the tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(num):\n",
    "    return num != num\n",
    "\n",
    "def preprocessTweet(tweet):\n",
    "    try:\n",
    "        tokenizer = TweetTokenizer()\n",
    "        stemmer = PorterStemmer()\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words_english = set(stopwords.words('english'))\n",
    "        stop_words_english.remove(\"not\") \n",
    "        if isNaN(tweet):\n",
    "            return tweet\n",
    "        tweet = tweet.encode('utf-16', 'surrogatepass').decode('utf-16')\n",
    "\n",
    "        #Remove Urls \n",
    "\n",
    "        tweet.replace(\"don't\",\"do not\")\n",
    "        tweet.replace(\"can't\",\"can not\")\n",
    "        tweet.replace(\"cant\",\"can not\")\n",
    "        tweet.replace(\"dont\",\"do not\")\n",
    "        tweet.replace(\"isn't\",\"is not\")\n",
    "        tweet.replace(\"won't\",\"will not\")\n",
    "        tweet.replace(\"shouldn't\",\"should not\")\n",
    "        tweet.replace(\"wouldn't\",\"would not\")\n",
    "\n",
    "        tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "        tweet = re.sub(r\"http\\S+\", \"\", tweet) \n",
    "\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "        #Capture retweets\n",
    "        if tokens[0] == \"RT\":\n",
    "            retweet = tokens[1][1:]\n",
    "            tokens.remove(\"RT\")\n",
    "            tokens = tokens[1:]\n",
    "\n",
    "        elif tokens[0] != \"RT\":\n",
    "            retweet = \"\"\n",
    "\n",
    "        #Remove the final ellipses in a tweet\n",
    "        tokens = tokens[:-1]\n",
    "\n",
    "        #Remove words with numbers in them\n",
    "        tweet = ' '.join(s for s in tokens if not any(c.isdigit() for c in s))\n",
    "        tweet = re.sub(r'([^a-zA-Z0-9])\\1{3,}',\"\", tweet)\n",
    "\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        remove_tokens = []\n",
    "\n",
    "        for token in tokens:\n",
    "            if token in string.punctuation:\n",
    "                remove_tokens.append(token)\n",
    "            if token[0] == \"#\":\n",
    "                remove_tokens.append(token)\n",
    "            if token[0] == \"@\":\n",
    "                remove_tokens.append(token)\n",
    "            if not token.isalpha():\n",
    "                remove_tokens.append(token)\n",
    "\n",
    "        cleaned_tokens = [token for token in tokens if token not in remove_tokens]\n",
    "\n",
    "        cleaned_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "        cleaned_tokens = [spell(token) for token in cleaned_tokens]\n",
    "        cleaned_tokens = [w.lower() for w in cleaned_tokens if not w.lower() in stop_words_english]\n",
    "        #cleaned_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "        return ' '.join(cleaned_tokens)\n",
    "    except:\n",
    "        return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to extract geolocation\n",
    "1. preprocess the user location by removing leading and trailing white space, converting it to lower case, add `india` if the text doesn't contain it.\n",
    "2. get_coordinate function returns the formatted address string, (longitude, latitude) for the preprocessed address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"m-app\")\n",
    "\n",
    "def preprocessLocation(location, geo_location):\n",
    "    loc = ''\n",
    "    try: \n",
    "        coordinates = []\n",
    "        if isNaN(geo_location) == False:\n",
    "            loc = location + ',' + geo_location\n",
    "        else:\n",
    "            loc = location\n",
    "        if isNaN(loc):\n",
    "            return loc\n",
    "\n",
    "        if 'india' not in loc.lower():\n",
    "            loc = loc + ', ' + 'India'\n",
    "        return loc.strip().lower()\n",
    "    except:\n",
    "        return loc\n",
    "\n",
    "def get_coordinate(loc):\n",
    "    try:\n",
    "        c = geolocator.geocode(loc)\n",
    "        if c != None:\n",
    "            return c\n",
    "    except :\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the scraped input data\n",
    "Process the input location from the scraped input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('election_march.csv').head(500)\n",
    "df['processed_geolocation'] = df.apply(lambda x: preprocessLocation(x['location'], x['geo_location']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the pre-processed input address into a set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoloc_set = set(df['processed_geolocation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through pre-processed input address set to extract [formated address, (latitude, longitude)] and append the values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 0\n",
    "geocode = {}\n",
    "for i in geoloc_set:\n",
    "    var = get_coordinate(i)\n",
    "#     print(\"iter {}: {}\".format(itr, var))\n",
    "    geocode[i] = var\n",
    "    itr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_state function extracts the state name from the input formated address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Andaman and Nicobar', 'Andhra Pradesh', 'Arunachal Pradesh', \\\n",
    "          'Assam', 'Bihar', 'Chandigarh', 'Chhattisgarh', 'Dadra and Nagar Haveli', \\\n",
    "          'Daman and Diu', 'Delhi', 'Goa', 'Gujarat', 'Haryana', 'Himachal Pradesh', \\\n",
    "          'Jammu and Kashmir', 'Jharkhand', 'Karnataka', 'Kerala', 'Lakshadweep', 'Madhya Pradesh', \\\n",
    "          'Maharashtra', 'Manipur', 'Meghalaya', 'Mizoram', 'Nagaland', 'Orissa', 'Puducherry', 'Punjab', \\\n",
    "          'Rajasthan', 'Sikkim', 'Tamil Nadu', 'Telangana', 'Tripura', 'Uttar Pradesh', 'Uttaranchal', \\\n",
    "          'West Bengal']\n",
    "\n",
    "from string import digits\n",
    "import re\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "def get_state(address):\n",
    "    tmp = ''\n",
    "    tmp_address = address.translate(remove_digits).strip('India')\n",
    "    for i in reversed(tmp_address.split(',')):\n",
    "        if len(i.strip(' ')) > 1:\n",
    "            tmp = i.strip(' ')\n",
    "            break\n",
    "    if tmp != '':\n",
    "        for state in states:\n",
    "            if (tmp.lower().replace(' ', '') in state.lower().replace(' ', '')) or (state.lower().replace(' ', '') in tmp.lower().replace(' ', '')):\n",
    "                return state\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function returns the Geo-location extracted through geocoder API for input pre-processed user address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(geolocation):\n",
    "    try:\n",
    "        return geocode[geolocation]\n",
    "    except:\n",
    "        return 'False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadoop/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "df1 = df[isNaN(df['processed_geolocation']) == False]\n",
    "df1['func'] = df1.apply(lambda x: func(x['processed_geolocation']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1[df1.apply(lambda x: not(x['func'] is None), axis=1)].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to extract the retweet username from the tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def get_retweet_user(text_retweet):\n",
    "    try:\n",
    "        user = re.match(r\"RT @(.*?):.*\", text_retweet.strip()).group(1)\n",
    "        if len(user) > 2:\n",
    "            return user\n",
    "        else:\n",
    "            return 'DUMMY'\n",
    "    except:\n",
    "        return  'DUMMY'\n",
    "\n",
    "def get_tweet(tweet, retweet):\n",
    "    if isNaN(tweet):\n",
    "        return retweet.strip('\\n').strip('\"');\n",
    "    else:\n",
    "        return tweet.strip('\\n').strip('\"');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the setiment for the tweet using TextBlob python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "from textblob import TextBlob\n",
    "\n",
    "def sentiment(text):\n",
    "    try:\n",
    "        return TextBlob(text).sentiment\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df2['tweet'] = df2.apply(lambda x: preprocessTweet(get_tweet(x['text'], x['text_retweet'])), axis=1)\n",
    "df2['Polarity'] = df2.apply(lambda x: sentiment(x['tweet'])[0], axis=1)\n",
    "#label\n",
    "sentiment_scores_tb = [round(TextBlob(text).sentiment.polarity, 3) for text in df2['tweet']]\n",
    "sentiment_category_tb = ['positive' if score > 0 \n",
    "                             else 'negative' if score < 0 \n",
    "                                 else 'neutral' \n",
    "                                     for score in sentiment_scores_tb]\n",
    "df2['sentiment'] = pd.DataFrame({'sentiment': sentiment_category_tb})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['address'] =  df2.apply(lambda x: x['func'].address, axis=1)\n",
    "df2['state'] =  df2.apply(lambda x: get_state(x['func'].address), axis=1)\n",
    "df2['lat'] =  df2.apply(lambda x: x['func'].latitude, axis=1)\n",
    "df2['longi'] =  df2.apply(lambda x: x['func'].longitude, axis=1)\n",
    "df2['retweet_user'] = df2.apply(lambda x: get_retweet_user(x['text_retweet']), axis=1)\n",
    "df2['weight'] = 1\n",
    "\n",
    "df2['party'] = df2.apply(lambda x: 'Congress' if ('congress' in str(x['tweet']).lower() or 'congress' in str(x['hashtags']).lower()) else 'BJP', axis=1)\n",
    "df2['congress'] = df2.apply(lambda x: 1 if x['party'] == 'Congress' else 0, axis=1)\n",
    "df2['bjp'] = df2.apply(lambda x: 1 if x['party'] == 'BJP' else 0, axis=1)\n",
    "df2['positive'] = df2.apply(lambda x: 1 if x['sentiment'] == 'positive' else 0, axis=1)\n",
    "df2['negative'] = df2.apply(lambda x: 1 if x['sentiment'] == 'negative' else 0, axis=1)\n",
    "df2['neutral'] = df2.apply(lambda x: 1 if x['sentiment'] == 'neutral' else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[df2.apply(lambda x: not(x['state'] is None), axis=1)][['created_date','screen_name','retweet_user','weight','state','lat','longi','party','sentiment','congress','bjp','positive','negative','neutral','tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('gData1.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
